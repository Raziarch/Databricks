{"cells":[{"cell_type":"code","source":["#%run ../utils/nginx_commons"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d9b439c-e730-4251-9158-6b5458951b02"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class NGINXDataTransform():\n  \"\"\"\n  transform, clean, and process data in preparation for nginx propensity modeling\n  write data to feature store to track features\n  read training, scoring, and evaluation data from feature store\n  \"\"\"\n  \n  def __init__(self\n               , train_dates=[\"2020-01-01\",\"2021-01-01\"]\n               , score_dates=[\"2022-01-01\"]\n               , dv=\"next_qtr_nginx_purchase\"\n               , env=\"dev\"\n               , primary_keys = [\"account_id\",\"calendar_date\"]\n               , database_prefix=\"mlops_nginx_example_\"\n               , raw_training_table_name=\"nginx_raw_training_data\"\n               , raw_validation_table_name=\"nginx_raw_validation_data\"\n               , preprocessed_training_table_name=\"nginx_preprocessed_training_data\"\n               , preprocessed_validation_table_name=\"nginx_preprocessed_validation_data\"\n               , raw_scoring_table_name=\"nginx_raw_scoring_data\"\n               , preprocessed_scoring_table_name=\"nginx_preprocessed_scoring_data\"\n               , training_fs_update=True\n               , scoring_fs_update=True\n               , limit=5000\n              ):\n    self.env = env\n    self.limit = limit\n    self.dv = dv\n    self.train_dates = train_dates\n    self.score_dates = score_dates\n    self.primary_keys = primary_keys\n    self.training_fs_update = training_fs_update\n    self.scoring_fs_update = scoring_fs_update\n    \n    # define db name & table names\n    self.database_prefix = database_prefix\n    self.raw_training_table_name = raw_training_table_name\n    self.raw_validation_table_name = raw_validation_table_name\n    self.preprocessed_training_table_name = preprocessed_training_table_name\n    self.preprocessed_validation_table_name = preprocessed_validation_table_name\n    self.raw_scoring_table_name = raw_scoring_table_name\n    self.preprocessed_scoring_table_name = preprocessed_scoring_table_name\n    \n    if(training_fs_update | scoring_fs_update):\n      # vars related to ede, used to write raw_training/socring data to ede, only for dev/qa env \n      if env == 'dev' or env == 'qa':\n        self.ede_schema = f\"{self.env}_DATA_SCIENCE\"\n        self.ede_pipeline = EDEPipeline(ede_schema = self.ede_schema)\n        \n      self.effective_date = str(date.today())  # initialize today as another primary key\n      self.update_feature_store(self.train_dates, self.score_dates)  # update the feature store\n    \n  def load_dv(self, dates):\n    dv_stmt = f\"select calendar_date, account_id, {self.dv} from EXP_MKTG.DATA_SCIENCE.NGINX_TARGET WHERE CALENDAR_DATE IN {(str(dates).replace('[','(').replace(']',')'))}\"\n    dv_df = pysh.safe_name(dbh.read_ede_data(dv_stmt)) # read data from EDE and safe name\n    dv_df = cast_col_type(dv_df, cols=[self.dv], new_col_type=\"boolean\") # int to boolean dtype conversion\n    return dv_df\n  \n  def load_raw_daily_snapshot_data(self, dates):\n    \"\"\"\n    extract or create all feature columns for set of dates from calendar_dataset, mintigo_target, latest years purchase qty/spend info from EDE\n    Safe naming (lower case) of all columns\n    \n    \"\"\"\n    \n    ## to save time while testing, specify features\n    raw_predictors = ['pct_leads_persona_level_total_past_3mo_individual_contributor', 'pct_leads_persona_function_past_3mo_network_admin_netops', 'pct_leads_persona_function_past_3mo_app_dev_devops', 'pct_leads_persona_function_past_12mo_network_admin_netops', 'pct_leads_persona_level_total_past_12mo_individual_contributor', 'pct_leads_persona_function_past_12mo_app_dev_devops', 'pct_leads_persona_function_past_12mo_information_security', 'pct_leads_persona_level_total_past_12mo_sr__manager_manager', 'pct_leads_persona_function_past_12mo_ciso_cio', 'pct_leads_persona_level_total_past_6mo_sr__director_director', 'leads_persona_total_past_3mo', 'leads_persona_total_past_12mo', 'num_person', 'last_6_month_gtm_campaign_nginx', 'last_6_month_gtm_campaign_secure_digital_exp', 'last_3_month_gtm_campaign_enable_modern_app', 'last_6_month_gtm_campaign_legacy', 'last_3months_digital_engagements_count', 'last_3months_web_visits', 'web_visits_per_person', 'last_6_month_gtm_campaign_service_provider', 'is_unix_linux_user', 'database_positions', 'has_intent_technology__data_center', 'is_data_warehouse_user', 'has_secured_connection', 'has_security_certification', 'has_intent_technology__data_center_and_managed_hosting', 'has_intent_technology__security_consulting', 'has_intent_technology__application_security', 'has_intent_technology__security', 'has_intent_technology__it_management', 'has_bigdata_solution', 'is_nosql_db_related_hiring', 'has_intent_technology__archiving', 'has_intent_technology__threat_hunting', 'has_mobile_website', 'has_blog', 'is_javascript_user', 'has_intent_technology__software_defined_data_center', 'has_intent_marketing__marketo', 'has_sql_db', 'has_education_titles', 'has_intent_technology__mobile_security', 'has_intent_technology__wireless_communications', 'total_csp_subtype_code', 'total_3_performance_degraded_severity_code', 'total_closed_sub_status', 'total_3_medium_priority_code', 'total_closed_status', 'total_sr_resolved', 'total_esrp_area', 'total_app_conn_proxy_area', 'total_ihealth_subtype_code', 'total_app_conn_svc_ctr_area', 'total_sdm_svc_provider_hot_ind', 'total_open_status', 'total_security_area', 'total_spend_brand_big_ip', 'last_5years_total_spend_forecast_group_traffic_management', 'total_spend_product_line_service', 'lifetime_months_complete_sales_cycle', 'total_spend_product_family_security', 'last_6months_std_booking_amount', 'total_renewal_qty_purchased', 'fiscal_qtr_code', 'q1_closed_opp_percentage', 'q2_closed_opp_percentage', 'q3_closed_opp_percentage', 'q4_closed_opp_percentage']\n    \n    # get calendar dataset daily snapshot data from EDE for specified dates\n    daily_snapshot_select_stmt = f\"select * from EXP_MKTG.DATA_SCIENCE.CALENDAR_DATASET WHERE CALENDAR_DATE IN {(str(dates).replace('[','(').replace(']',')'))}\"\n    daily_snapshot_df = pysh.safe_name(dbh.read_ede_data(daily_snapshot_select_stmt))\n    \n    # isolate old marketing features \n    old_marketing_select_stmt = f\"select * from exp_mktg.data_science.marketing_target WHERE CALENDAR_DATE IN {(str(dates).replace('[','(').replace(']',')'))}\"\n    old_marketing_select_df = (type_to_type(pysh.safe_name(dbh.read_ede_data(old_marketing_select_stmt)), 'decimal', 'double'))\n    old_marketing_cols = [col for col in old_marketing_select_df.columns if col not in self.primary_keys]\n\n    # remove date/timestamp cols, cols related to DVs, and old marketing features\n    remove_cols = [col for col, dtype in daily_snapshot_df.dtypes if dtype in ('date','timestamp') or 'next' in col or 'country' in col or 'state' in col] + old_marketing_cols\n\n    daily_snapshot_df = daily_snapshot_df.drop(*remove_cols)\n\n    \n    # get mintigo dataset data from EDE for specified dates\n    mintigo_select_stmt = f\"select * from EXP_MKTG.DATA_SCIENCE.MINTIGO_TARGET WHERE CALENDAR_DATE IN {(str(dates).replace('[','(').replace(']',')'))}\"\n    mintigo_df = (pysh.safe_name(dbh.read_ede_data(mintigo_select_stmt))\n                  .withColumnRenamed(\"salesforce_account_id\",\"account_id\")\n                 )\n    remove_cols = [col for col, dtype in mintigo_df.dtypes if (dtype=='string' or dtype=='date') and col not in (self.primary_keys)]\n    mintigo_df = mintigo_df.drop(*remove_cols)\n    \n    # get marketing feature data from EDE for specified dates\n    marketing_select_stmt = f\"select * from exp_mktg.data_science.marketing_feature_account_target WHERE CALENDAR_DATE IN {(str(dates).replace('[','(').replace(']',')'))}\"\n    marketing_df = pysh.safe_name(dbh.read_ede_data(marketing_select_stmt))\n    \n    # get last X years spend and quantity data from EDE for specified dates\n    last_x_years_purchase_select_stmt = f\"select * from EXP_MKTG.DATA_SCIENCE.PRODUCT_LIFETIME_AND_LAST_YEARS_TARGET WHERE CALENDAR_DATE IN {(str(dates).replace('[','(').replace(']',')'))}\"\n    last_x_years_purchase_df = pysh.safe_name(dbh.read_ede_data(last_x_years_purchase_select_stmt))\n    \n    # get dv\n    dv_df = self.load_dv(dates)\n    \n    # get features for modeling that are natively in datasets\n    marketing_features = marketing_df.columns \n    raw_daily_snapshot_features = [col for col in daily_snapshot_df.columns if col not in marketing_features]\n    raw_mintigo_features = mintigo_df.columns\n    last_x_years_purchase_features = last_x_years_purchase_df.columns\n    \n    # join all datasets together \n    df = (daily_snapshot_df\n          .select(self.primary_keys+raw_daily_snapshot_features)\n          .join(other=(mintigo_df\n                       .select(self.primary_keys+raw_mintigo_features)\n                      )\n                , on=self.primary_keys\n                , how=\"left\"\n               )\n          .join(other=marketing_df.select(self.primary_keys + marketing_features)\n                , on=self.primary_keys\n                , how=\"left\"\n               )\n          .join(other=last_x_years_purchase_df.select(self.primary_keys + last_x_years_purchase_features)\n                , on=self.primary_keys\n                , how=\"left\"\n               )\n          .select(self.primary_keys+raw_predictors) # remove after testing\n          .join(other=dv_df\n                , on=self.primary_keys\n                , how=\"left\"\n               )\n         )\n\n    return df\n  \n  \n  def handle_decimal_types(self, df):\n    \"\"\"change all decimal data types to double data type\"\"\"\n    return type_to_type(df, 'decimal', 'double')\n\n    \n  \n  def get_accounts_to_remove(self):\n    \"\"\"\n    Return df of account ids that represent a known population of accounts that should not be part of modeling based on name conventions \n    Such as F5 test accounts, nginx, marketplace, dev central accounts etc.\n    And population of accounts that haven't made an active purchase in last 5 years.\n    \"\"\"\n    acct_firma_select_stmt = f\"select * from PRD_ENT_RAW.SALESFORCE.ACCOUNT\"\n    acct_firma_df = pysh.safe_name(dbh.read_ede_data(acct_firma_select_stmt))\n    \n    remove_accts = (acct_firma_df\n                    .filter((F.lower(F.col(\"name\")).contains(\"(marketplace)\"))\n                            | (F.lower(\"name\").contains(\"nginx\"))\n                            | (F.lower(\"name\").contains(\"f5\"))\n                            | (F.col(\"account_org_name_c\").isin(\"Sales Ops Test\",\"M2S Test Account\"))\n                           )\n                    .select(\"id\")\n                    .withColumnRenamed(\"id\",\"account_id\")\n                   )\n    \n    return remove_accts\n  \n\n    \n  \n  def preprocess_data(self, df):\n    \"\"\"preprocess data such as null handling, type conversion\"\"\"\n    \n    # convert decimal types to double\n    df = self.handle_decimal_types(df) \n    \n    # list of columns\n    raw_features = [col for col in df.columns if col not in self.primary_keys]\n    med_replace_list = ['avg_days_between_product_opps',\n                       'avg_days_between_renewal_opps',\n                       'avg_days_between_opps',\n                       'avg_days_complete_product_sales_cycle',\n                       'avg_days_complete_renewal_sales_cycle',\n                       'avg_days_complete_sales_cycle',\n                       'avg_time_between_successes',\n                       'avg_days_resolve_sr',\n                       'avg_days_close_sr',\n                       'avg_days_til_first_response',\n                       'avg_sr_status_chg',\n                       'avg_times_in_queue',\n                       'days_since_last_opened_sr',\n                       'days_since_last_resolved_sr',\n                       'days_since_last_closed_sr',\n                       'months_since_last_opened_sr',\n                       'months_since_last_resolved_sr',\n                       'months_since_last_closed_sr']\n    \n    \n    max_replace_list = ['days_since_last_product_purchase',\n                       'months_since_last_product_purchase',\n                       'days_since_last_renewal_purchase',\n                       'months_since_last_renewal_purchase',\n                       'days_since_last_purchase',\n                       'months_since_last_purchase',\n                       'days_since_first_product_purchase',\n                       'months_since_first_product_purchase',\n                       'days_since_first_renewal_purchase',\n                       'months_since_first_renewal_purchase',\n                       'days_since_first_purchase',\n                       'months_since_first_purchase',\n                       'days_since_last_offline_success',\n                       'days_since_last_online_success',\n                       'days_since_last_security_success',\n                       'days_since_last_eng',\n                       'avg_time_between_successes',\n                       'days_since_first_swp_purchase',\n                       'days_since_latest_swp_purchase',\n                       'months_since_first_swp_purchase',\n                       'months_since_latest_swp_purchase',\n                       'days_since_first_awf_silverline_purchase',\n                       'days_since_latest_awf_silverline_purchase',\n                       'months_since_first_awf_silverline_purchase',\n                       'months_since_latest_awf_silverline_purchase',\n                       'months_since_first_vpr_purchase',\n                       'months_since_latest_vpr_purchase']\n    \n    categorical_list = ['account_type','theater','industry_grouping','industry','revenue_segment','fiscal_period_desc','fiscal_qtr_code','org_industry']\n    \n    ftrs_med_replace_list = [col for col in raw_features if col in med_replace_list]\n    ftrs_max_replace_list = [col for col in raw_features if col in max_replace_list]\n    ftrs_cat_custom_list = [col for col in raw_features if col in categorical_list]\n    ftrs_zero_replace = [self.dv] + list(set(raw_features) - set(max_replace_list) - set(med_replace_list) - set(categorical_list))\n    \n    # null replacement,  one hot encoding, int-->boolean dtype of categoricals\n    processed_df = null_replace(df, cols=ftrs_cat_custom_list, replace_type='custom',custom='None')\n    processed_df = one_hot_encoding(processed_df, cols=ftrs_cat_custom_list)\n    int_to_bool = [col for col in processed_df.columns for cat in ftrs_cat_custom_list if cat in col]\n    processed_df = cast_col_type(processed_df, cols=int_to_bool, new_col_type=\"boolean\")\n    \n    # median, max, and zero null replace\n    processed_df = null_replace(processed_df, cols=ftrs_med_replace_list, replace_type='median')\n    processed_df = null_replace(processed_df, cols=ftrs_max_replace_list, replace_type='max')\n    processed_df = null_replace(processed_df, cols=ftrs_zero_replace, replace_type='zero')\n    \n    # safe name\n    processed_df = pysh.safe_name(processed_df)\n    \n    # remove string features or original categorical features\n    remove_cols = [col for col,dtype in processed_df.dtypes if dtype=='string' and col not in self.primary_keys]\n    processed_df = processed_df.drop(*remove_cols)\n    \n    return processed_df\n    \n  def train_validation_split(self, df):\n    \"\"\"\n    Split dataframe into training and validation frames\n    Assumes both X and y dataframes have same dates\n    Validation frame uses last date in list of calendar dates\n    Validation frame uses 20% randomized set of accounts\n    \n    Return 2 data frames: (training_df, validation_df)\n    \"\"\"\n\n    # split accounts into train & validation sets\n    validation_accts = (df\n                        .select(\"account_id\")\n                        .dropDuplicates()\n                        .sample(withReplacement=False, fraction=0.2, seed=123)\n                       )\n\n    # # split calendar dates into train and validation sets \n    calendar_dates = [r['calendar_date'] for r in df.select(\"calendar_date\").dropDuplicates().collect() if r['calendar_date'] != '2022-07-01'] # get available calendar dates\n    calendar_dates.sort() # order calendar dates asc\n    validation_dates = calendar_dates.pop(-1) # use latest date as validation date\n    training_dates = calendar_dates # use remaining dates as train dates\n\n    # # create training and validation dataframe\n    training_df = (df\n                     .filter(F.col(\"calendar_date\").isin(training_dates))\n                     .join(other=validation_accts\n                           , on=[\"account_id\"]\n                           , how=\"left_anti\"\n                          )\n                    )\n    validation_df = (df\n                       .filter(F.col(\"calendar_date\") == validation_dates )\n                       .join(other=validation_accts\n                           , on=[\"account_id\"]\n                           , how=\"inner\"\n                          )\n                      )  \n\n    return  (training_df, validation_df)\n  \n  def update_training_and_validation_fs_table(self, dates, table_name=('raw_train','preprocessed_train', 'raw_validation', 'preprocessed_validation')):\n    \"\"\"\n    update raw & pre processed training and validation feature store table with features and label\n    \"\"\"\n    # get accounts to remove \n    remove_accts_df = self.get_accounts_to_remove()\n    \n    # load raw data\n    raw_df = (self.load_raw_daily_snapshot_data(dates)\n              .withColumn(\"effective_date\", F.lit(self.effective_date))\n              .join(other=remove_accts_df\n                    , on=[\"account_id\"]\n                    , how=\"left_anti\"\n                   )\n             )\n    \n    # preprocess raw data\n    preprocessed_df = (self.preprocess_data(raw_df)\n                       .withColumn(\"effective_date\", F.lit(self.effective_date))\n                      )\n    \n    # train & validation split on preprocessed data\n    preprocessed_training_df, preprocessed_validation_df = self.train_validation_split(df=preprocessed_df)\n    \n    # match train & validation split populations on raw data\n    raw_training_df = (raw_df\n                       .join(other=(preprocessed_training_df\n                                    .select(self.primary_keys)\n                                    .dropDuplicates()\n                                   )\n                             , on=self.primary_keys\n                             , how=\"inner\"\n                            )\n                       \n                      )\n    \n    raw_validation_df = (raw_df\n                       .join(other=(preprocessed_validation_df\n                                    .select(self.primary_keys)\n                                    .dropDuplicates()\n                                   )\n                             , on=self.primary_keys\n                             , how=\"inner\"\n                            )\n                         \n                      )\n\n    \n    # write raw and processed training and validation data to fs\n    self.write_data_to_fs(raw_training_df, table_name[0])\n    # write raw data to ede\n    if (self.env != \"prod\"):\n      self.ede_pipeline.write_to_ede(df=raw_training_df\n                                     , mode=\"overwrite\"\n                                     , ede_table_name=table_name[0].split(\".\")[1]\n                                    )\n    self.write_data_to_fs(preprocessed_training_df, table_name[1])\n    self.write_data_to_fs(raw_validation_df, table_name[2])\n    self.write_data_to_fs(preprocessed_validation_df, table_name[3])\n    \n  \n  def update_scoring_fs_table(self, dates, table_name=('raw_score','preprocessed_score')):\n    \"\"\"\n    update scoring feature store table with features only \n    this is an example: in reality, it should be different from training fs\n    \"\"\"\n    \n    # get accounts to remove \n    remove_accts_df = self.get_accounts_to_remove()\n    \n    # load raw data\n    scoring_data = (self.load_raw_daily_snapshot_data(dates)\n                    .join(other=remove_accts_df\n                          , on=[\"account_id\"]\n                          , how=\"left_anti\"\n                         )\n                    .withColumn(\"effective_date\", F.lit(self.effective_date))\n                   )\n    # preprocess raw data\n    preprocessed_scoring_data = self.preprocess_data(scoring_data)\n    # drop dv from raw and processed data\n    scoring_data = (scoring_data\n                   .drop(self.dv) #remove dv field\n                   )\n    preprocessed_scoring_data = (preprocessed_scoring_data\n                                 .withColumn(\"effective_date\", F.lit(self.effective_date))\n                                 .drop(self.dv) #remove dv field\n                                )\n    \n    # write raw and processed data to fs\n    self.write_data_to_fs(scoring_data, table_name[0])\n    # write raw data to ede\n    if (self.env != \"prod\"):\n      self.ede_pipeline.write_to_ede(df=scoring_data\n                                     , mode=\"overwrite\"\n                                     , ede_table_name=table_name[0].split(\".\")[1]\n                                    )\n    self.write_data_to_fs(preprocessed_scoring_data, table_name[1])\n    \n  def write_data_to_fs(self, df, table_name, description=''):\n    \"\"\"\n    write df to feature store\n    \"\"\" \n    # create database if not existing\n    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.database_prefix+self.env}\")\n    \n    fs = FeatureStoreClient()\n    try:\n      print(\"Reading in table \" + table_name)\n      df_fs = fs.read_table(table_name)\n      df_merge = unify_col_type(df_fs, df)  # unify col type before merge\n      # when the table exists, merge it\n      fs.write_table(\n        name=table_name,\n        df=df_merge,\n        mode=\"merge\")\n    except:\n      print(\"creating table\")\n      # when not exists, create the table\n      fs.create_table(\n        name=table_name,\n        primary_keys=['calendar_date', 'account_id','effective_date'],\n        df = df,\n        description='nginx data')\n      \n      \n  def update_feature_store(self, train_dates, score_date):\n    \"\"\"\n    update the training and scoring feature score table\n    \"\"\"\n\n\n    # update training data\n    if self.training_fs_update:\n      self.update_training_and_validation_fs_table(table_name = (f\"{self.database_prefix + self.env}.{self.raw_training_table_name}\"\n                                                                 , f\"{self.database_prefix + self.env}.{self.preprocessed_training_table_name}\"\n                                                                 , f\"{self.database_prefix + self.env}.{self.raw_validation_table_name}\"\n                                                                 , f\"{self.database_prefix + self.env}.{self.preprocessed_validation_table_name}\"\n                                                                )\n                                                   , dates=train_dates\n                                                  )\n    \n    if self.scoring_fs_update:\n      self.update_scoring_fs_table(table_name = (f\"{self.database_prefix + self.env}.{self.raw_scoring_table_name}\"\n                                                 , f\"{self.database_prefix+self.env}.{self.preprocessed_scoring_table_name}\"\n                                                )\n                                   , dates=score_date\n                                  )\n    \n\n\n  \n  def get_training_data(self, predictors, is_raw=False, primary_keys=['calendar_date','account_id']):\n    \"\"\"\n    read training data from fs\n    returns X and y pyspark dataframes\n    \"\"\"\n    \n    ### Add call to preprocessing \n    \n    fs = FeatureStoreClient()\n    if is_raw==True:\n      table_name = f\"{self.database_prefix + self.env}.{self.raw_training_table_name}\"\n    if is_raw==False:\n      table_name = f\"{self.database_prefix + self.env}.{self.preprocessed_training_table_name}\"\n    \n    latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {table_name}\")\n                             .collect()[0][\"latest_effective_date\"])\n    \n    # extract the training data on the latest_effective_date\n    training_data = fs.read_table(table_name).filter(F.col(\"effective_date\") == latest_effective_date)\n    return training_data.select(primary_keys+predictors), training_data.select(primary_keys+[self.dv])  # return X, y as dataframe\n  \n  def get_validation_data(self, predictors, is_raw=False, primary_keys=['calendar_date','account_id']):\n    \"\"\"\n    read validation data from fs\n    return X, y pyspark dataframes\n    \"\"\"\n    \n    ### Add call to preprocessing \n    \n    fs = FeatureStoreClient()\n    if is_raw==True:\n      table_name = f\"{self.database_prefix + self.env}.{self.raw_validation_table_name}\"\n    if is_raw==False:\n      table_name = f\"{self.database_prefix + self.env}.{self.preprocessed_validation_table_name}\"\n    \n    latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {table_name}\")\n                             .collect()[0][\"latest_effective_date\"])\n    \n    # extract the training data on the latest_effective_date\n    validation_data = fs.read_table(table_name).filter(F.col(\"effective_date\") == latest_effective_date)\n    return validation_data.select(primary_keys+predictors), validation_data.select(primary_keys+[self.dv])  # return X, y as dataframe\n  \n  \n  def get_scoring_data(self, predictors, date, is_raw=False, primary_keys=['calendar_date','account_id']):\n    \"\"\"\n    read scoring data from fs\n    \"\"\"\n    \n    fs = FeatureStoreClient()\n    if is_raw==True:\n      table_name = f\"{self.database_prefix + self.env}.{self.raw_scoring_table_name}\"\n    if is_raw==False:\n      table_name = f\"{self.database_prefix+self.env}.{self.preprocessed_scoring_table_name}\"\n    \n    latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {table_name} where calendar_date = '{date}'\")\n                             .collect()[0][\"latest_effective_date\"])\n    \n    # extract the training data on the latest_effective_date\n    scoring_data = (fs\n                    .read_table(table_name)\n                    .filter(F.col(\"effective_date\") == latest_effective_date)\n                    .filter(F.col(\"calendar_date\")==date)\n                   )\n    \n    # check existence of predictors in score df\n    unexisting_columns = [col for col in predictors if col not in scoring_data.columns]\n    print(\"Cols missing from score data: \",unexisting_columns)\n    existing_predictors = [col for col in predictors if col not in unexisting_columns]\n    return scoring_data.select(primary_keys+existing_predictors)\n  \n  \n  def get_evaluation_data(self, is_raw=False, primary_keys=['calendar_date','account_id']):\n    \"\"\"\n    read evaluation data from fs, including all features to evaluate different models\n    \"\"\"\n    fs = FeatureStoreClient()\n    if is_raw==True:\n      validation_table_name = f\"{self.database_prefix + self.env}.{self.raw_validation_table_name}\"\n      training_table_name = f\"{self.database_prefix + self.env}.{self.raw_training_table_name}\"\n    if is_raw==False:\n      validation_table_name = f\"{self.database_prefix + self.env}.{self.preprocessed_validation_table_name}\"\n      training_table_name = f\"{self.database_prefix + self.env}.{self.preprocessed_training_table_name}\"\n      \n    \n    validation_latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {validation_table_name}\")\n                             .collect()[0][\"latest_effective_date\"])\n    training_latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {training_table_name}\")\n                             .collect()[0][\"latest_effective_date\"])\n    \n    # extract the training and evaluation data on the latest_effective_date\n    validation_evaluation_data = fs.read_table(validation_table_name).filter(F.col(\"effective_date\") == validation_latest_effective_date)\n    training_evaluation_data = fs.read_table(training_table_name).filter(F.col(\"effective_date\") == training_latest_effective_date)\n    \n    \n    return training_evaluation_data, validation_evaluation_data\n    \n  \n  \n  \n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Class: NGINXDataTransform","showTitle":true,"inputWidgets":{},"nuid":"6591cf3b-b1d4-4188-a072-367df64b1c83"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"nginx_data_transform","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":24704457090440}},"nbformat":4,"nbformat_minor":0}
