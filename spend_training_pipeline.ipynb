{"cells":[{"cell_type":"code","source":["#%run ../utils/nginx_commons"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07d84811-bca9-4390-9880-4d975af491e1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#%run ../data/nginx_data_transform"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14cb1376-6255-4b24-aeb3-d19a5100a09b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#%run ../EDE/nginx_write_to_ede"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4247cb65-6d08-433d-91dc-70993fafc16c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import os.path\nimport h2o"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0178c009-35bf-44ac-9498-3384689828a6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class NGINXTrainingPipeline():\n  \"\"\"\n  Hyper parameter tune\n  Refit model using all data\n  Saved trained model in MLflow\n  In dev: model is saved in experiment\n  In qa: model is excalated to stating, which will be evaluated in evaluation_pipeline\n  In prod: TBD\n  \"\"\"\n  def __init__(self\n               , model_name\n               , experiment_id\n               , env=\"dev\"\n               , dv=\"next_qtr_nginx_purchase\"\n               , primary_keys=[\"calendar_date\",\"account_id\"]\n               , model_path=\"/dbfs/mlops_nginx\"\n               , fs_update=True\n               , database_prefix=\"mlops_nginx_example_\"\n               , training_shap_individual_contributions_table_name=\"nginx_training_shap_individual_contributions_data\"\n               , validation_shap_individual_contributions_table_name=\"nginx_validation_shap_individual_contributions_data\"\n               , training_permutation_importance_table_name=\"nginx_training_permutation_importance_data\"\n               , validation_permutation_importance_table_name=\"nginx_validation_permutation_importance_data\"\n               , training_variable_importance_table_name=\"nginx_training_variable_importance_data\"\n               , train_data_table_name=\"nginx_train_modeling_data\"\n               , validation_data_table_name=\"nginx_validation_modeling_data\"\n              ):\n    self.model_name = model_name\n    self.experiment_id = experiment_id\n    self.env = env    # dev, qa, and prod\n    self.dv = dv\n    self.primary_keys = primary_keys\n    self.model_path = model_path\n    \n    # vars related to feature store\n    self.database_prefix = database_prefix\n    self.fs_update = fs_update\n    self.training_shap_individual_contributions_table_name = training_shap_individual_contributions_table_name\n    self.validation_shap_individual_contributions_table_name = validation_shap_individual_contributions_table_name\n    self.training_permutation_importance_table_name = training_permutation_importance_table_name\n    self.validation_permutation_importance_table_name = validation_permutation_importance_table_name\n    self.training_variable_importance_table_name = training_variable_importance_table_name\n    self.train_data_table_name = train_data_table_name\n    self.validation_data_table_name = validation_data_table_name\n    \n    # vars related to ede \n    if env == 'dev' or env == 'qa':\n      self.ede_schema = f\"{self.env}_DATA_SCIENCE\"\n    elif env == 'prod':\n      self.ede_schema = \"DATA_SCIENCE\"\n    self.ede_pipeline = EDEPipeline(ede_schema = self.ede_schema)\n    \n    \n    if(fs_update):\n      self.effective_date = str(date.today())  # initialize today as another primary key\n#       self.update_feature_store(self.train_dates, self.score_dates)  # update the feature store\n    \n  def update_feature_store(self, model, training_df, validation_df):\n    \"\"\"\n    Update feature store for tables related to individual shap contributions,  permutation variable importance, and (gini) variable importance for training and validation populations\n    Input: model object, h2o dataframe\n    \"\"\"\n    \n    # update training data\n    self.update_train_and_validation_fs_table(model=model\n                                              , table_name=self.train_data_table_name\n                                              , df=training_df\n                                              , population_type=\"training\"\n                                             )\n    \n    # update validation data\n    self.update_train_and_validation_fs_table(model=model\n                                              , table_name=self.validation_data_table_name\n                                              , df=validation_df\n                                              , population_type=\"validation\"\n                                             )\n    \n    # update shap for training and validation populations in FS and EDE\n    self.update_individual_contributions_fs_table(df=training_df\n                                                  , model=model\n                                                  , table_name=self.training_shap_individual_contributions_table_name\n                                                  , population_type=\"training\"\n                                                 )\n    self.update_individual_contributions_fs_table(df=validation_df\n                                                  , model=model\n                                                  , table_name=self.validation_shap_individual_contributions_table_name\n                                                  , population_type=\"validation\"\n                                                 )\n    \n    # update permutation variable importance for training and validation populations\n    self.update_permutation_importance_fs_table(df=training_df\n                                                , model=model\n                                                , table_name=self.training_permutation_importance_table_name\n                                                , population_type=\"training\"\n                                               )\n    self.update_permutation_importance_fs_table(df=validation_df\n                                                , model=model\n                                                , table_name=self.validation_permutation_importance_table_name\n                                                , population_type=\"validation\"\n                                               )\n    \n    # update variable importance for training populations\n    self.update_training_variable_importance_fs_table(model=model\n                                                      , table_name=self.training_variable_importance_table_name\n                                                     )\n  \n    \n\n    \n  def update_train_and_validation_fs_table(self, model, table_name, df, population_type):\n    \"\"\"Update train and validation tables and tie to the model they're related with\"\"\"\n    # Set h2o context\n    hc = H2OContext.getOrCreate()\n    \n    # get model_id from model object\n    model_id = model.actual_params['model_id'] \n    \n    # turn h2o dfs to spark\n    df_pys = (hc.asSparkFrame(h2oFrame=df)\n              .withColumn(\"population_type\",F.lit(population_type))\n              .withColumn(\"model_id\",F.lit(model_id))\n              .withColumn(\"effective_date\",F.lit(self.effective_date))\n            )\n    df_pys = type_to_type(df_pys, 'tinyint', 'double')\n    \n    # write data to fs\n    self.write_data_to_fs(df=df_pys\n                          , is_var_imp_fs=False\n                          , table_name=f\"{self.database_prefix + self.env}.{table_name}\"\n                         )\n    \n    # write data to EDE\n    if (population_type != \"validation\"):\n      fs = FeatureStoreClient()\n      self.ede_pipeline.write_to_ede(df=fs.read_table(f\"{self.database_prefix + self.env}.{table_name}\").filter(F.col(\"effective_date\")==self.effective_date)\n                                     , mode=\"overwrite\"\n                                     , ede_table_name=table_name\n                                     , fixed_cols=['calendar_date', 'account_id','effective_date', 'population_type', 'model_id']\n                                    )\n    \n    \n    \n    \n    \n    \n  \n  def update_training_variable_importance_fs_table(self, model, table_name):\n    \"\"\"\n    Update variable importance contributions fs\n    https://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html\n    Input: h2o df, model, text name of table\n    \"\"\"\n    # Set h2o context\n    hc = H2OContext.getOrCreate()\n    \n    # get model_id from model object\n    model_id = model.actual_params['model_id']\n    \n    # calculate permutation importance\n    var_imp_pys = (spark\n                   .createDataFrame(model.varimp(use_pandas=True).reset_index())\n                   .withColumn(\"effective_date\", F.lit(self.effective_date))\n                   .withColumn(\"model_id\", F.lit(model_id))\n                   .withColumn(\"population_type\", F.lit(\"training\"))\n                  )\n    \n    # write variable  importance  to fs\n    self.write_data_to_fs(df=var_imp_pys\n                          , is_var_imp_fs=True\n                          , table_name=f\"{self.database_prefix + self.env}.{table_name}\"\n                         )\n    \n    # write variable importance to EDE\n    self.ede_pipeline.write_to_ede(df=var_imp_pys\n                                   , mode=\"overwrite\"\n                                   , ede_table_name=table_name\n                                  )\n    \n    \n  def update_permutation_importance_fs_table(self, df, model, population_type, table_name):\n    \"\"\"\n    Update permutation importance contributions fs\n    Input: h2o df, model, population type to add label, text name of table\n    \"\"\"\n    # Set h2o context\n    hc = H2OContext.getOrCreate()\n    \n    # get model_id from model object\n    model_id = model.actual_params['model_id']\n    \n    # calculate permutation importance\n    perm_imp_pys = (pysh.safe_name(spark\n                                   .createDataFrame(model.permutation_importance(df, use_pandas=True).reset_index())\n                                   .withColumn(\"effective_date\", F.lit(self.effective_date))\n                                   .withColumn(\"model_id\", F.lit(model_id))\n                                   .withColumn(\"population_type\", F.lit(population_type))\n                                  )\n                   )\n    \n    # write permutation feature importance  to fs\n    self.write_data_to_fs(df=perm_imp_pys\n                          , is_var_imp_fs=True\n                          , table_name=f\"{self.database_prefix + self.env}.{table_name}\"\n                         )\n    \n    # write permutation feature importance to EDE\n    if (population_type != \"validation\"):\n      self.ede_pipeline.write_to_ede(df=perm_imp_pys\n                                     , mode=\"overwrite\"\n                                     , ede_table_name=table_name\n                                    )\n    \n    \n  \n  def update_individual_contributions_fs_table(self, df, model, population_type, table_name): \n    \"\"\"\n    Update individual shap contributions + bias term fs\n    Input: h2o df and h2o model, text name of table, text defining population type (i.e. training, validation, etc.)\n    \"\"\"\n    # Set h2o context\n    hc = H2OContext.getOrCreate()\n    \n    # get model_id from model object\n    model_id = model.actual_params['model_id']\n    \n    # calculate shap contributions and relate back to the customer\n    individual_contributions_h2o = df[['account_id','calendar_date']].cbind(model.predict_contributions(df))\n    \n    # convert to pyspark frame , handle datatypes , and add necessary primary key columns\n    individual_contributions_pys = hc.asSparkFrame(h2oFrame=individual_contributions_h2o)\n    individual_contributions_pys = (type_to_type(pysh.safe_name(individual_contributions_pys), 'tinyint', 'double')\n                                    .withColumn(\"effective_date\", F.lit(self.effective_date))\n                                    .withColumn(\"model_id\", F.lit(model_id))\n                                    .withColumn(\"population_type\", F.lit(population_type))\n                                   )\n    individual_contributions_pys.cache().count()\n    \n    # write individual contribution predictions to fs\n    self.write_data_to_fs(df=individual_contributions_pys\n                          , is_var_imp_fs=False\n                          , table_name=f\"{self.database_prefix + self.env}.{table_name}\"\n                         )\n    \n#     # comment out, because we don't write training/validation individual contribution predictions to EDE\n#       self.ede_pipeline.write_to_ede(df=individual_contributions_pys\n#                                      , mode=\"overwrite\"\n#                                      , ede_table_name=table_name\n#                                     )\n    individual_contributions_pys.unpersist()\n  \n    \n  def write_data_to_fs(self, df, is_var_imp_fs, table_name=\"nginx_\", description=''):\n    \"\"\"\n    write df to feature store\n    \"\"\"\n    # create database if not existing\n    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.database_prefix+self.env}\")\n    \n    print(\"write data to table \" + table_name)\n    fs = FeatureStoreClient()\n    try:\n      print(\"Reading in table \" + table_name)\n      df_fs = fs.read_table(table_name)\n      df_merge = unify_col_type(df_fs, df)  # unify col type before merge\n      spark.sql(f\"delete from {table_name} where effective_date = '{self.effective_date}'\")\n      print(\"Remove existing records for date:\", self.effective_date)\n      # when the table exists, merge it\n      fs.write_table(\n        name=table_name,\n        df=df_merge,\n        mode=\"merge\")\n    except:\n      print(\"creating table\")\n      # when not exists, create the table\n      if is_var_imp_fs==True:\n        fs.create_table(\n          name=table_name,\n          primary_keys=['effective_date', 'population_type','variable'], # keep 1 set of records for each effective_date, remove \"model_id\"\n          df = df,\n          description='nginx data')\n      else:\n        fs.create_table(\n          name=table_name,\n          primary_keys=self.primary_keys + ['effective_date', 'population_type'], # keep 1 set of records for each effective_date, remove \"model_id\"\n          df = df,\n          description='nginx data')\n    print(table_name + \"updated successfully\")\n    \n  def get_shap_individual_contribution_data(self, model_id, population_type=\"training\", primary_keys=['calendar_date','account_id']):\n    \"\"\"\n    read training data from fs\n    \"\"\"\n    \n    ### Add call to preprocessing \n    \n    fs = FeatureStoreClient()\n    if population_type=='validation':\n      table_name = f\"{self.database_prefix + self.env}.{self.validation_shap_individual_contributions_table_name}\"\n    if population_type=='training':\n      table_name = f\"{self.database_prefix + self.env}.{self.training_shap_individual_contributions_table_name}\"\n      \n      \n    latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {table_name} where model_id = '{model_id}'\")\n                             .collect()[0][\"latest_effective_date\"])\n    \n    # extract the shap contribution data on the latest_effective_date\n    shap_contributions_data = (fs\n                               .read_table(table_name)\n                               .filter(F.col(\"effective_date\") == latest_effective_date)\n                               .filter(F.col(\"model_id\") == model_id)\n                              )\n    return shap_contributions_data\n\n  \n  def get_training_var_importance_data(self, model_id):\n    \"\"\"\n    read var importance data from fs\n    inputs: model_id relates to the model path\n    \"\"\"\n    \n    ### Add call to preprocessing \n    \n    fs = FeatureStoreClient()\n    table_name = f\"{self.database_prefix + self.env}.{self.training_variable_importance_table_name}\"\n    \n    latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {table_name} where model_id = '{model_id}'\")\n                             .collect()[0][\"latest_effective_date\"])\n    \n    # extract the training data on the latest_effective_date\n    var_importance = (fs\n                       .read_table(table_name)\n                       .filter(F.col(\"effective_date\") == latest_effective_date)\n                       .filter(F.col(\"model_id\") == model_id)\n                      )\n    return var_importance\n  \n  def get_permutation_var_importance_data(self, model_id, population_type=\"training\"):\n    \"\"\"\n    read permutation importance data from fs\n    inputs: population_type should be 'validation' or training, and model_id relates to the model path\n    \"\"\"\n    \n    ### Add call to preprocessing \n    \n    fs = FeatureStoreClient()\n    if population_type=='validation':\n      table_name = f\"{self.database_prefix + self.env}.{self.validation_permutation_importance_table_name}\"\n    if population_type=='training':\n      table_name = f\"{self.database_prefix + self.env}.{self.training_permutation_importance_table_name}\"\n    \n    latest_effective_date = (spark.sql(f\"select max(effective_date) as latest_effective_date from {table_name} where model_id = '{model_id}'\")\n                             .collect()[0][\"latest_effective_date\"])\n    \n    # extract the training data on the latest_effective_date\n    perm_importance = (fs\n                       .read_table(table_name)\n                       .filter(F.col(\"effective_date\") == latest_effective_date)\n                       .filter(F.col(\"model_id\") == model_id)\n                      )\n    return perm_importance\n  \n  def train(self, training_df, validation_df, params):\n    \"\"\"\n    Train using train validation split using params provided\n    Input: h2o training and validation dfs, dictionary params\n    \"\"\"\n    \n    # isolate predictors\n    remove = self.primary_keys \n    predictors = [x for x in training_df.columns if x not in remove and x != self.dv]\n    \n    # set h2o context \n    hc = H2OContext.getOrCreate()\n    \n  \n    # train\n    model = H2OXGBoostEstimator(seed=123\n                                  , **params\n                                 )\n    model.train(x=predictors\n                  , y=self.dv\n                  , training_frame = training_df\n                  , validation_frame = validation_df\n                  , verbose=True\n                  )\n    \n    return model\n    \n    \n    \n  def get_metrics(self, model, df):\n    \"\"\"Get aucpr, auc, F1, F2, and MCC evaluation based on model and dataset\"\"\"\n    performance = model.model_performance(df)\n    \n    # H2O Performance Documentation: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html\n\n      ### Choose evaluation metric that evaluates the predicted class and is robust to imbalance data aka does not consider relative size of True Negatives\n      ##### MCC - evaluates based on predicted class, good for imbalance data\n      ##### F2 - evaluates based on predicted class, good for giving more weight on recall\n      ##### AUCPR - evaluates based on predicted class, good for imbalance data\n      \n    performance_dict = {'aucpr':performance.aucpr()\n                        , 'auc':performance.auc()\n                        , 'F1':performance.F1()[0][1] # weighted harmonic mean of precision & recall -- equal weight\n                        , 'F2':performance.F2()[0][1] # weighted harmonic mean of precision & recall -- more weight to recall , penalize false negative higher than false positive \n                        , 'mcc':performance.mcc()[0][1]\n                       }\n    return performance_dict\n    \n                  \n  def train_and_tune(self, training_X_df, training_y_df, validation_X_df, validation_y_df):\n    \"\"\" Train with parameter tuning\"\"\"\n    \n    def objective(params):\n      \"\"\"Train H2O XGBoost model using validation dataframe and parameters returning metric that should be minimized\"\"\"\n      # https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html\n      \n      with mlflow.start_run(experiment_id=experiment_id, run_name=\"CHILD_RUN\", nested=True) as child_run:\n        params['max_depth'] = int(params['max_depth'])\n        params['ntrees'] = int(params['ntrees'])\n        model = H2OXGBoostEstimator(seed=123\n                                    , **params\n                                   )\n\n        model.train(x=predictors\n                    , y=self.dv\n                    , training_frame = training_df_h2o\n                    , validation_frame = validation_df_h2o\n                    , verbose=True\n                    )\n\n        train_performance_dict = self.get_metrics( model=model, df=training_df_h2o)\n        train_performance_dict = {f\"train_{k}\" :v for k,v in train_performance_dict.items()}\n        val_performance_dict = self.get_metrics( model=model, df=validation_df_h2o)\n        val_performance_dict = {f\"validation_{k}\" :v for k,v in val_performance_dict.items()}\n\n        metrics_dict = {**val_performance_dict,**train_performance_dict}\n        mlflow.log_metrics(metrics_dict)\n        tags = {'run_id':child_run.info.run_id\n               }\n        mlflow.set_tags(tags)\n\n      return {'loss': -val_performance_dict['validation_aucpr'], 'status': STATUS_OK}\n\n  \n\n    # Set h2o context\n    hc = H2OContext.getOrCreate()\n    \n    # train & validation split\n    training_df = (training_X_df\n               .join(other=training_y_df\n                     , on=['calendar_date','account_id']\n                     , how='inner'\n                    )\n              )\n    validation_df = (validation_X_df\n                   .join(other=validation_y_df\n                         , on=['calendar_date','account_id']\n                         , how='inner'\n                        )\n                  )\n  \n  \n    # define predictors as all columns in dataframe except for DV & defined primary keys  \n    remove = self.primary_keys \n    predictors = [x for x in training_df.columns if x not in remove and x != self.dv]\n    training_df_h2o = hc.asH2OFrame(sparkFrame=training_df) \n    validation_df_h2o = hc.asH2OFrame(sparkFrame=validation_df) \n    \n    # define search space\n    search_space = {'max_depth': hp.choice('max_depth', range(2,20,2)),\n                    'ntrees': hp.choice('ntrees',range(30,100,5)),\n                    'reg_lambda': hp.loguniform('reg_lambda', 0,5),\n                    'reg_alpha': hp.quniform('alpha',0,10,1),\n                    'learn_rate': hp.quniform('learn_rate', 0.05, 0.5 , 0.05),\n                    'scale_pos_weight': hp.quniform('scale_pos_wight',5,13,.5), #useful for imbalanced data - count(neg obs) / count(pos obs) or if very imbalance then conservatively, sqrt(count(neg obs) / count(pos obs))\n                    'col_sample_rate_per_tree':hp.uniform('col_sample_rate_per_tree',0.5,1)\n                   }\n    \n    with mlflow.start_run(experiment_id=self.experiment_id, run_name=\"test\", nested=True) as run:\n      best = fmin(fn=objective\n                  , space=search_space\n                  , algo=tpe.suggest\n                  , max_evals=7 # keep small for testing purposes\n                  , trials=Trials()\n                 )\n      \n      best_param = space_eval(search_space, best)  # best may include index, use space_val to map it to values\n      mlflow.log_params(best_param)\n    \n    \n      best_model = self.train(training_df=training_df_h2o\n                              , validation_df=validation_df_h2o\n                              , params=best_param)\n      \n      # calculate and log metrics for best model for training and validation data\n      train_performance_dict = self.get_metrics( model=best_model, df=training_df_h2o)\n      train_performance_dict = {f\"train_{k}\" :v for k,v in train_performance_dict.items()}\n      val_performance_dict = self.get_metrics( model=best_model, df=validation_df_h2o)\n      val_performance_dict = {f\"validation_{k}\" :v for k,v in val_performance_dict.items()}\n\n      metrics_dict = {**val_performance_dict,**train_performance_dict}\n      mlflow.log_metrics(metrics_dict)\n      \n      # create confusion matrix based on F1 and log for train/validation sets\n      training_confusion_matrix = best_model.confusion_matrix(train=True).table.as_data_frame()\n      validation_confusion_matrix = best_model.confusion_matrix(valid=True).table.as_data_frame()\n\n      training_confusion_matrix_path = \"/tmp/nginx_training_confusion_matrix.csv\"\n      training_confusion_matrix.to_csv(training_confusion_matrix_path, index=False)\n      mlflow.log_artifact(training_confusion_matrix_path)\n\n      validation_confusion_matrix_path = \"/tmp/nginx_validation_confusion_matrix.csv\"\n      training_confusion_matrix.to_csv(validation_confusion_matrix_path, index=False)\n      mlflow.log_artifact(validation_confusion_matrix_path)\n\n      # create model signature and log it with model\n      model_signature = infer_signature(training_df.select(predictors), training_df.select(self.dv))\n\n      # get model path\n      model_abs_path = h2o.save_model(model = best_model, path=self.model_path)\n      model_rel_path = os.path.basename(model_abs_path)\n\n      # load the model\n      saved_model = h2o.load_model(model_abs_path)\n\n      mlflow.h2o.log_model(h2o_model=saved_model, artifact_path=model_rel_path, signature=model_signature)\n      model_tags = {\"model_path\": model_rel_path,\n                    \"run_id\":run.info.run_id\n                   }\n      mlflow.set_tags(model_tags)\n      \n      # if TRUE save shap feature contributions, permutation variable imp, and variable imp to feature store \n      if self.fs_update == True:\n        self.update_feature_store(model=best_model, training_df=training_df_h2o, validation_df=validation_df_h2o)\n    \n      \n    \n    # model registry\n    if(self.env == \"qa\"):\n      print(\"Register model\")\n      model_uri = \"runs:/{run_id}/{artifact_path}\".format(run_id=run.info.run_id, artifact_path=model_rel_path)\n      model_details = mlflow.register_model(model_uri=model_uri, name=self.model_name)   # create a model version\n      self.wait_until_ready(self.model_name, model_details.version)\n\n      print(\"transition from None to Staging\")\n      client = MlflowClient()\n      client.transition_model_version_stage(name=self.model_name, version=model_details.version,\n                                            stage=\"Staging\")\n      \n      \n  # Wait until the model is ready\n  def wait_until_ready(self, model_name, model_version):\n    client = MlflowClient()\n    for _ in range(60):\n      model_version_details = client.get_model_version(\n        name=model_name,\n        version=model_version)\n      \n      status = ModelVersionStatus.from_string(model_version_details.status)\n      print(\"Model status: %s\" % ModelVersionStatus.to_string(status))\n      if status == ModelVersionStatus.READY:\n        break\n      time.sleep(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e2a4a59-7f91-46d9-9d25-37764f95acdc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# tables = ['mlops_nginx_example_dev.nginx_training_permutation_importance_data'\n#           , 'mlops_nginx_example_dev.nginx_training_shap_individual_contributions_data'\n#           , 'mlops_nginx_example_dev.nginx_training_variable_importance_data'\n#           , 'mlops_nginx_example_dev.nginx_validation_permutation_importance_data'\n#           , 'mlops_nginx_example_dev.nginx_validation_shap_individual_contributions_data'\n#          ]\n\n\n# for table in tables:\n#   print(table)\n#   try:\n#     spark.sql(f\"DELETE FROM {table}\") \n#   except:\n#     print(\"Failed delete\")\n#   try:\n#     spark.sql(f\"VACUUM {table}\")\n#   except:\n#     print(\"Failed vacuum\")\n#   try:\n#     spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n#   except:\n#     print(\"Failed drop\")\n#   print()\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d2b1534-a9c6-4703-a6ed-f8a861c05fae","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b911880a-0f62-4924-9da8-d5cbfcb4ddc6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"nginx_training_pipeline","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":24704457090378}},"nbformat":4,"nbformat_minor":0}
